{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf8c9ce-ab73-4814-aaf5-98d16685ac5f",
   "metadata": {},
   "source": [
    "# Python for DE - Extract and Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea546ec-2d26-4d13-8320-f83397bddd96",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "##### 1. Types of Systems Python can Connect to for Read and Write\n",
    "##### 2. Exercise Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a85a8-0986-4602-af2e-098c4ae43275",
   "metadata": {},
   "source": [
    "## Types of Systems Python can Connect to for Read and Write\n",
    "1. **Database Drivers**\n",
    "    - You can connect to a database with the required credentials using certain Python libraries\n",
    "    - Examples include: sqlite3, psycopg2, duckdb\n",
    "\n",
    "2. **Cloud SDKs**\n",
    "    - Most of the cloud providers provide their own SDK (software development kit) to work with the cloud services\n",
    "    - Examples include `boto3` for AWS and `gsutil` for GCP\n",
    "    - The SDKs can be used to either extract or load data from a cloud storage like S3 bucket, GCP Cloud Store\n",
    "\n",
    "3. **APIs**\n",
    "    - Some systems expose their data via APIs\n",
    "    - You can make a HTTPS request using Python's `request` library to work with them\n",
    "\n",
    "4. **Files**\n",
    "    - Python can read multiple types of files JSON, XML, Parquet, CSV etc\n",
    "\n",
    "5. **SFTP/FTP**\n",
    "    - FTP: File Transfer Protocol\n",
    "    - SFTP: SSH File Transfer Protocol aka Secure FTP\n",
    "    - Both of them are used to work with files on a server\n",
    "    - You can browse, upload or download files from or to your local computer via the FTP\n",
    "    - The key difference is that FTP uses unencrypted channels\n",
    "\n",
    "6. **Queing System**\n",
    "    - Systems that queue data and Python has libs to write these to say Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0493cc4a-d9bf-48a4-a73a-0a8ffe5b5ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state_code</th>\n",
       "      <th>datetime_created</th>\n",
       "      <th>datetime_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14409</td>\n",
       "      <td>franca</td>\n",
       "      <td>SP</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9790</td>\n",
       "      <td>sao bernardo do campo</td>\n",
       "      <td>SP</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1151</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8775</td>\n",
       "      <td>mogi das cruzes</td>\n",
       "      <td>SP</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>13056</td>\n",
       "      <td>campinas</td>\n",
       "      <td>SP</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  zipcode                   city state_code  \\\n",
       "0            1    14409                 franca         SP   \n",
       "1            2     9790  sao bernardo do campo         SP   \n",
       "2            3     1151              sao paulo         SP   \n",
       "3            4     8775        mogi das cruzes         SP   \n",
       "4            5    13056               campinas         SP   \n",
       "\n",
       "      datetime_created     datetime_updated  \n",
       "0  2017-10-18 00:00:00  2017-10-18 00:00:00  \n",
       "1  2017-10-18 00:00:00  2017-10-18 00:00:00  \n",
       "2  2017-10-18 00:00:00  2017-10-18 00:00:00  \n",
       "3  2017-10-18 00:00:00  2017-10-18 00:00:00  \n",
       "4  2017-10-18 00:00:00  2017-10-18 00:00:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://github.com/josephmachado/python_essentials_for_data_engineers/raw/refs/heads/main/data/customers.csv\"\n",
    "df_customers = pd.read_csv(url)\n",
    "df_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a62f6e-07cb-4c95-95a2-8a193b806cf6",
   "metadata": {},
   "source": [
    "### Exercise Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ee26dc-7edf-4f58-8277-6b3ac83b790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract: Process to pull data from Source system\n",
    "# Load: Process to write data to a destination system\n",
    "\n",
    "# Common upstream & downstream systems\n",
    "# OLTP Databases: Postgres, MySQL, sqlite3, etc\n",
    "# OLAP Databases: Snowflake, BigQuery, Clickhouse, DuckDB, etc\n",
    "# Cloud data storage: AWS S3, GCP Cloud Store, Minio, etc\n",
    "# Queue systems: Kafka, Redpanda, etc\n",
    "# API\n",
    "# Local disk: csv, excel, json, xml files\n",
    "# SFTP\\FTP server\n",
    "\n",
    "# Databases: When reading or writing to a database we use a database driver. Database drivers are libraries that we can use to read or write to a database.\n",
    "# Question: How do you read data from a sqlite3 database and write to a DuckDB database?\n",
    "# Hint: Look at importing the database libraries for sqlite3 and duckdb and create connections to talk to the respective databases\n",
    "\n",
    "# Fetch data from the SQLite Customer table\n",
    "\n",
    "# Insert data into the DuckDB Customer table\n",
    "\n",
    "# Hint: Look for Commit and close the connections\n",
    "# Commit tells the DB connection to send the data to the database and commit it, if you don't commit the data will not be inserted\n",
    "\n",
    "# We should close the connection, as DB connections are expensive\n",
    "\n",
    "import sqlite3\n",
    "import duckdb\n",
    "\n",
    "# Connect to a SQLite DB (Creates one if does not exist)\n",
    "conn = sqlite3.connect(\"customers.db\")\n",
    "cursor = conn.cursor()\n",
    "df_customers.to_sql(\"customers\", conn, if_exists=\"replace\", index=False)\n",
    "conn.commit()\n",
    "\n",
    "# Fetch data from the SQLite Customer table\n",
    "# cursor.execute(\"SELECT * FROM customers\")\n",
    "# rows = cursor.fetchall()\n",
    "\n",
    "# We could use the INSERT statements but it is not recommended\n",
    "# Duckdb has native Pandas capabilities therefore we will use that\n",
    "# Insert data into the DuckDB Customer table\n",
    "\n",
    "# 1. read from Sqlite DB as pandas df\n",
    "df_customers_sql = pd.read_sql_query(\"SELECT * FROM customers\", conn)\n",
    "\n",
    "# 2. Create or open the target DB\n",
    "duckdb_conn = duckdb.connect(\"customers.duckdb\")\n",
    "\n",
    "# 3. Insert into DuckDB table\n",
    "duckdb_conn.register(\"df_customers_sql\", df_customers_sql)  # Register the DataFrame\n",
    "duckdb_conn.execute(\"CREATE TABLE IF NOT EXISTS customers AS SELECT * FROM df_customers_sql\")\n",
    "\n",
    "duckdb_conn.commit()\n",
    "\n",
    "# Close connections\n",
    "conn.close()\n",
    "duckdb_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f6ee417-8bef-45bd-8f50-0956a381c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud storage\n",
    "# Question: How do you read data from the S3 location given below and write the data to a DuckDB database?\n",
    "# Data source: https://docs.opendata.aws/noaa-ghcn-pds/readme.html station data at path \"csv.gz/by_station/ASN00002022.csv.gz\"\n",
    "# Hint: Use boto3 client with UNSIGNED config to access the S3 bucket\n",
    "# Hint: The data will be zipped you have to unzip it and decode it to utf-8\n",
    "\n",
    "# AWS S3 bucket and file details\n",
    "bucket_name = \"noaa-ghcn-pds\"\n",
    "file_key = \"csv.gz/by_station/ASN00002022.csv.gz\"\n",
    "# Create a boto3 client with anonymous access\n",
    "\n",
    "# Download the CSV file from S3\n",
    "# Decompress the gzip data\n",
    "# Read the CSV file using csv.reader\n",
    "# Connect to the DuckDB database (assume WeatherData table exists)\n",
    "\n",
    "# Insert data into the DuckDB WeatherData table\n",
    "\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "s3_client = boto3.client('s3', config = Config(signature_version=UNSIGNED))\n",
    "# 1. Download the file\n",
    "s3_client.download_file(bucket_name, file_key, \"./ASN00002022.csv.gz\")\n",
    "\n",
    "# 2. Decompress the gzip data\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "with gzip.open(\"ASN00002022.csv.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    # 3. Read the CSV file using csv.reader\n",
    "    csv_reader = csv.reader(f)\n",
    "\n",
    "    # rest of the remaning rows \n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "   # csv_reader is a generator function therefore it resumes processing from the last point\n",
    "\n",
    "\n",
    "# 4. Connect to the DuckDB database (assume WeatherData table exists)\n",
    "duckdb_conn = duckdb.connect(\"WeatherData.duckdb\")\n",
    "# Create table if not exists\n",
    "duckdb_conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS WeatherData (\n",
    "        ID STRING,\n",
    "        DATE STRING,\n",
    "        ELEMENT STRING,\n",
    "        VALUE INTEGER,\n",
    "        M_FLAG STRING,\n",
    "        Q_FLAG STRING,\n",
    "        S_FLAG STRING,\n",
    "        OBS_TIME STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into DuckDB\n",
    "duckdb_conn.executemany(\"INSERT INTO WeatherData VALUES (?, ?, ?, ?, ?, ?, ?, ?)\", data)\n",
    "\n",
    "\n",
    "#duckdb_conn.execute(\"FROM WeatherData\").fetchall()\n",
    "duckdb_conn.commit() # not necessary\n",
    "\n",
    "duckdb_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6885f4-7fde-41c2-9831-214036608d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "# Question: How do you read data from the CoinCap API given below and write the data to a DuckDB database?\n",
    "# URL: \"https://api.coincap.io/v2/exchanges\"\n",
    "# Hint: use requests library\n",
    "\n",
    "\n",
    "\n",
    "# Fetch data from the CoinCap API\n",
    "# Connect to the DuckDB database\n",
    "\n",
    "# Insert data into the DuckDB Exchanges table\n",
    "# Prepare data for insertion\n",
    "# Hint: Ensure that the data types of the data to be inserted is compatible with DuckDBs data column types in ./setup_db.py\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"https://api.coincap.io/v2/exchanges\"\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    df_response = pd.DataFrame(data['data'])\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "conn = duckdb.connect(\"exchanges.db\")\n",
    "conn.execute(\"CREATE TABLE IF NOT EXISTS exchanges AS SELECT * FROM df_response;\")\n",
    "#conn.execute(\"SELECT * FROM exchanges LIMIT 5;\").fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3450202-b766-4d3d-a864-088245019edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data successfully written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Local disk\n",
    "# Question: How do you read a CSV file from local disk and write it to a database?\n",
    "# Look up open function with csvreader for python\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# Connect to the database (creates file if not exists)\n",
    "conn = sqlite3.connect(\"local_files.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# File name\n",
    "file_name = \"m13_arrivals_sample.csv\"\n",
    "\n",
    "# Open CSV file and read headers\n",
    "with open(file_name, \"rt\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    headers = next(csv_reader)  # Read the first row as headers\n",
    "    \n",
    "    # Create table dynamically based on CSV headers\n",
    "    columns = \", \".join([f\"{col} TEXT\" for col in headers])  # All columns as TEXT\n",
    "    cursor.execute(f\"CREATE TABLE IF NOT EXISTS some_csv ({columns});\")\n",
    "    \n",
    "    # Insert data into the table\n",
    "    placeholders = \", \".join([\"?\"] * len(headers))  # Create placeholders (?, ?, ?)\n",
    "    insert_query = f\"INSERT INTO some_csv VALUES ({placeholders})\"\n",
    "    \n",
    "    cursor.executemany(insert_query, csv_reader)  # Insert all rows\n",
    "\n",
    "# Commit and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"CSV data successfully written to the database.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "944ae9b4-6025-4669-9baa-1596243a8923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "# Web scraping\n",
    "# Questions: Use beatiful soup to scrape the below website and print all the links in that website\n",
    "# URL of the website to scrape\n",
    "url = 'https://example.com'\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# Find all anchor tags and extract href attribute\n",
    "links = [a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\")]\n",
    "\n",
    "# Print all links\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e16fa12-33ee-4164-9383-5d1be0c22a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 52.52,\n",
       " 'longitude': 13.419998,\n",
       " 'generationtime_ms': 0.045418739318847656,\n",
       " 'utc_offset_seconds': 0,\n",
       " 'timezone': 'GMT',\n",
       " 'timezone_abbreviation': 'GMT',\n",
       " 'elevation': 38.0,\n",
       " 'hourly_units': {'time': 'iso8601', 'temperature_2m': 'Â°C'},\n",
       " 'hourly': {'time': ['2025-01-29T00:00',\n",
       "   '2025-01-29T01:00',\n",
       "   '2025-01-29T02:00',\n",
       "   '2025-01-29T03:00',\n",
       "   '2025-01-29T04:00',\n",
       "   '2025-01-29T05:00',\n",
       "   '2025-01-29T06:00',\n",
       "   '2025-01-29T07:00',\n",
       "   '2025-01-29T08:00',\n",
       "   '2025-01-29T09:00',\n",
       "   '2025-01-29T10:00',\n",
       "   '2025-01-29T11:00',\n",
       "   '2025-01-29T12:00',\n",
       "   '2025-01-29T13:00',\n",
       "   '2025-01-29T14:00',\n",
       "   '2025-01-29T15:00',\n",
       "   '2025-01-29T16:00',\n",
       "   '2025-01-29T17:00',\n",
       "   '2025-01-29T18:00',\n",
       "   '2025-01-29T19:00',\n",
       "   '2025-01-29T20:00',\n",
       "   '2025-01-29T21:00',\n",
       "   '2025-01-29T22:00',\n",
       "   '2025-01-29T23:00',\n",
       "   '2025-01-30T00:00',\n",
       "   '2025-01-30T01:00',\n",
       "   '2025-01-30T02:00',\n",
       "   '2025-01-30T03:00',\n",
       "   '2025-01-30T04:00',\n",
       "   '2025-01-30T05:00',\n",
       "   '2025-01-30T06:00',\n",
       "   '2025-01-30T07:00',\n",
       "   '2025-01-30T08:00',\n",
       "   '2025-01-30T09:00',\n",
       "   '2025-01-30T10:00',\n",
       "   '2025-01-30T11:00',\n",
       "   '2025-01-30T12:00',\n",
       "   '2025-01-30T13:00',\n",
       "   '2025-01-30T14:00',\n",
       "   '2025-01-30T15:00',\n",
       "   '2025-01-30T16:00',\n",
       "   '2025-01-30T17:00',\n",
       "   '2025-01-30T18:00',\n",
       "   '2025-01-30T19:00',\n",
       "   '2025-01-30T20:00',\n",
       "   '2025-01-30T21:00',\n",
       "   '2025-01-30T22:00',\n",
       "   '2025-01-30T23:00',\n",
       "   '2025-01-31T00:00',\n",
       "   '2025-01-31T01:00',\n",
       "   '2025-01-31T02:00',\n",
       "   '2025-01-31T03:00',\n",
       "   '2025-01-31T04:00',\n",
       "   '2025-01-31T05:00',\n",
       "   '2025-01-31T06:00',\n",
       "   '2025-01-31T07:00',\n",
       "   '2025-01-31T08:00',\n",
       "   '2025-01-31T09:00',\n",
       "   '2025-01-31T10:00',\n",
       "   '2025-01-31T11:00',\n",
       "   '2025-01-31T12:00',\n",
       "   '2025-01-31T13:00',\n",
       "   '2025-01-31T14:00',\n",
       "   '2025-01-31T15:00',\n",
       "   '2025-01-31T16:00',\n",
       "   '2025-01-31T17:00',\n",
       "   '2025-01-31T18:00',\n",
       "   '2025-01-31T19:00',\n",
       "   '2025-01-31T20:00',\n",
       "   '2025-01-31T21:00',\n",
       "   '2025-01-31T22:00',\n",
       "   '2025-01-31T23:00',\n",
       "   '2025-02-01T00:00',\n",
       "   '2025-02-01T01:00',\n",
       "   '2025-02-01T02:00',\n",
       "   '2025-02-01T03:00',\n",
       "   '2025-02-01T04:00',\n",
       "   '2025-02-01T05:00',\n",
       "   '2025-02-01T06:00',\n",
       "   '2025-02-01T07:00',\n",
       "   '2025-02-01T08:00',\n",
       "   '2025-02-01T09:00',\n",
       "   '2025-02-01T10:00',\n",
       "   '2025-02-01T11:00',\n",
       "   '2025-02-01T12:00',\n",
       "   '2025-02-01T13:00',\n",
       "   '2025-02-01T14:00',\n",
       "   '2025-02-01T15:00',\n",
       "   '2025-02-01T16:00',\n",
       "   '2025-02-01T17:00',\n",
       "   '2025-02-01T18:00',\n",
       "   '2025-02-01T19:00',\n",
       "   '2025-02-01T20:00',\n",
       "   '2025-02-01T21:00',\n",
       "   '2025-02-01T22:00',\n",
       "   '2025-02-01T23:00',\n",
       "   '2025-02-02T00:00',\n",
       "   '2025-02-02T01:00',\n",
       "   '2025-02-02T02:00',\n",
       "   '2025-02-02T03:00',\n",
       "   '2025-02-02T04:00',\n",
       "   '2025-02-02T05:00',\n",
       "   '2025-02-02T06:00',\n",
       "   '2025-02-02T07:00',\n",
       "   '2025-02-02T08:00',\n",
       "   '2025-02-02T09:00',\n",
       "   '2025-02-02T10:00',\n",
       "   '2025-02-02T11:00',\n",
       "   '2025-02-02T12:00',\n",
       "   '2025-02-02T13:00',\n",
       "   '2025-02-02T14:00',\n",
       "   '2025-02-02T15:00',\n",
       "   '2025-02-02T16:00',\n",
       "   '2025-02-02T17:00',\n",
       "   '2025-02-02T18:00',\n",
       "   '2025-02-02T19:00',\n",
       "   '2025-02-02T20:00',\n",
       "   '2025-02-02T21:00',\n",
       "   '2025-02-02T22:00',\n",
       "   '2025-02-02T23:00',\n",
       "   '2025-02-03T00:00',\n",
       "   '2025-02-03T01:00',\n",
       "   '2025-02-03T02:00',\n",
       "   '2025-02-03T03:00',\n",
       "   '2025-02-03T04:00',\n",
       "   '2025-02-03T05:00',\n",
       "   '2025-02-03T06:00',\n",
       "   '2025-02-03T07:00',\n",
       "   '2025-02-03T08:00',\n",
       "   '2025-02-03T09:00',\n",
       "   '2025-02-03T10:00',\n",
       "   '2025-02-03T11:00',\n",
       "   '2025-02-03T12:00',\n",
       "   '2025-02-03T13:00',\n",
       "   '2025-02-03T14:00',\n",
       "   '2025-02-03T15:00',\n",
       "   '2025-02-03T16:00',\n",
       "   '2025-02-03T17:00',\n",
       "   '2025-02-03T18:00',\n",
       "   '2025-02-03T19:00',\n",
       "   '2025-02-03T20:00',\n",
       "   '2025-02-03T21:00',\n",
       "   '2025-02-03T22:00',\n",
       "   '2025-02-03T23:00',\n",
       "   '2025-02-04T00:00',\n",
       "   '2025-02-04T01:00',\n",
       "   '2025-02-04T02:00',\n",
       "   '2025-02-04T03:00',\n",
       "   '2025-02-04T04:00',\n",
       "   '2025-02-04T05:00',\n",
       "   '2025-02-04T06:00',\n",
       "   '2025-02-04T07:00',\n",
       "   '2025-02-04T08:00',\n",
       "   '2025-02-04T09:00',\n",
       "   '2025-02-04T10:00',\n",
       "   '2025-02-04T11:00',\n",
       "   '2025-02-04T12:00',\n",
       "   '2025-02-04T13:00',\n",
       "   '2025-02-04T14:00',\n",
       "   '2025-02-04T15:00',\n",
       "   '2025-02-04T16:00',\n",
       "   '2025-02-04T17:00',\n",
       "   '2025-02-04T18:00',\n",
       "   '2025-02-04T19:00',\n",
       "   '2025-02-04T20:00',\n",
       "   '2025-02-04T21:00',\n",
       "   '2025-02-04T22:00',\n",
       "   '2025-02-04T23:00'],\n",
       "  'temperature_2m': [4.7,\n",
       "   4.4,\n",
       "   4.8,\n",
       "   4.6,\n",
       "   4.7,\n",
       "   5.2,\n",
       "   5.2,\n",
       "   5.3,\n",
       "   5.7,\n",
       "   6.7,\n",
       "   7.7,\n",
       "   7.7,\n",
       "   8.6,\n",
       "   9.2,\n",
       "   9.3,\n",
       "   8.7,\n",
       "   7.9,\n",
       "   7.5,\n",
       "   7.2,\n",
       "   6.8,\n",
       "   6.3,\n",
       "   6.0,\n",
       "   5.4,\n",
       "   5.1,\n",
       "   4.7,\n",
       "   4.2,\n",
       "   3.7,\n",
       "   3.3,\n",
       "   3.2,\n",
       "   3.0,\n",
       "   2.7,\n",
       "   2.5,\n",
       "   2.9,\n",
       "   3.9,\n",
       "   5.1,\n",
       "   6.2,\n",
       "   7.0,\n",
       "   7.6,\n",
       "   7.7,\n",
       "   7.5,\n",
       "   7.0,\n",
       "   6.6,\n",
       "   6.3,\n",
       "   6.3,\n",
       "   6.2,\n",
       "   6.0,\n",
       "   5.6,\n",
       "   5.2,\n",
       "   3.8,\n",
       "   3.2,\n",
       "   2.9,\n",
       "   2.9,\n",
       "   3.1,\n",
       "   3.1,\n",
       "   2.9,\n",
       "   2.6,\n",
       "   2.5,\n",
       "   2.8,\n",
       "   3.3,\n",
       "   3.9,\n",
       "   4.6,\n",
       "   5.4,\n",
       "   5.3,\n",
       "   5.1,\n",
       "   4.1,\n",
       "   3.6,\n",
       "   2.7,\n",
       "   1.9,\n",
       "   1.3,\n",
       "   0.8,\n",
       "   0.5,\n",
       "   0.4,\n",
       "   0.3,\n",
       "   0.2,\n",
       "   0.0,\n",
       "   -0.2,\n",
       "   -0.4,\n",
       "   -0.6,\n",
       "   -0.6,\n",
       "   -0.9,\n",
       "   -1.0,\n",
       "   -0.8,\n",
       "   -0.2,\n",
       "   0.4,\n",
       "   1.0,\n",
       "   1.7,\n",
       "   2.1,\n",
       "   1.7,\n",
       "   1.2,\n",
       "   0.8,\n",
       "   0.5,\n",
       "   0.2,\n",
       "   -0.1,\n",
       "   -0.2,\n",
       "   -0.2,\n",
       "   -0.2,\n",
       "   -0.1,\n",
       "   -0.2,\n",
       "   -0.3,\n",
       "   -0.4,\n",
       "   -0.5,\n",
       "   -0.7,\n",
       "   -0.8,\n",
       "   -0.8,\n",
       "   -0.8,\n",
       "   -0.8,\n",
       "   -0.6,\n",
       "   -0.3,\n",
       "   -0.1,\n",
       "   0.0,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   -0.0,\n",
       "   -0.1,\n",
       "   -0.2,\n",
       "   -0.3,\n",
       "   -0.3,\n",
       "   -0.3,\n",
       "   -0.3,\n",
       "   -0.4,\n",
       "   -0.5,\n",
       "   -0.6,\n",
       "   -0.8,\n",
       "   -0.9,\n",
       "   -1.1,\n",
       "   -1.2,\n",
       "   -1.3,\n",
       "   -1.3,\n",
       "   -1.3,\n",
       "   -1.0,\n",
       "   -0.2,\n",
       "   0.9,\n",
       "   1.6,\n",
       "   1.9,\n",
       "   1.9,\n",
       "   1.7,\n",
       "   1.3,\n",
       "   0.6,\n",
       "   0.0,\n",
       "   -0.4,\n",
       "   -0.9,\n",
       "   -1.1,\n",
       "   -1.0,\n",
       "   -0.7,\n",
       "   -0.5,\n",
       "   -0.5,\n",
       "   -0.5,\n",
       "   -0.5,\n",
       "   -0.6,\n",
       "   -0.6,\n",
       "   -0.6,\n",
       "   -0.5,\n",
       "   -0.3,\n",
       "   -0.1,\n",
       "   0.4,\n",
       "   1.0,\n",
       "   1.4,\n",
       "   1.6,\n",
       "   1.7,\n",
       "   1.6,\n",
       "   1.2,\n",
       "   0.7,\n",
       "   0.2,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "params = {\n",
    "    \"latitude\": 52.52,  # Berlin\n",
    "    \"longitude\": 13.41,\n",
    "    \"hourly\": \"temperature_2m\"\n",
    "}\n",
    "requests.get(url, params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7221d978-4a5b-4ded-bc4a-3fe6402f69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Results:\n",
      "- readline error when saving interactive command history when history file path is a symlink to a relative path (#129453)\n",
      "- _Py_TryIncrefCompareStackRef incorrectly listed as non-escaping in cases generator (#129452)\n",
      "- gh-129438: Update ``--enable-experimental-jit`` section with install requirements (#129450)\n",
      "- gh-111495: Add PyFile tests (#129449)\n",
      "- gh-129250: allow pickle instances of generic classes (#129446)\n",
      "Page 2 Results:\n",
      "- Nested virtual environment support in site/venv modules (#129445)\n",
      "- [3.12] gh-129345: null check for indent syslogmodule (GH-129348) (#129443)\n",
      "- [3.13] gh-129345: null check for indent syslogmodule (GH-129348) (#129442)\n",
      "- test_instrumentation failing randomly on free-threading CI (#129441)\n",
      "- [3.10] gh-119461: Fix ThreadedVSOCKSocketStreamTest (GH-129171) (#129440)\n",
      "Page 3 Results:\n",
      "- Python 3.13.1-jit : issue when compiling from source (#129438)\n",
      "- gh-129354: Use PyErr_FormatUnraisable() function (#129435)\n",
      "- gh-101944: Clarify PyModule_AddObjectRef() documentation (#129433)\n",
      "- Crash with `PYTHON_LLTRACE=4` due to presence of `PyDictKeysObject` on stack (#129432)\n",
      "- gh-100239: specialize right shift ops on ints (#129431)\n"
     ]
    }
   ],
   "source": [
    "# Base URL\n",
    "url = \"https://api.github.com/repos/python/cpython/issues\"\n",
    "\n",
    "# Pagination settings\n",
    "per_page = 5  # Number of results per page\n",
    "page = 1  # Start with page 1\n",
    "\n",
    "while True:\n",
    "    # Define request parameters\n",
    "    params = {\"per_page\": per_page, \"page\": page}\n",
    "\n",
    "    # Make API request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # Break loop if no more data\n",
    "        if not data:\n",
    "            print(\"No more data to fetch.\")\n",
    "            break\n",
    "\n",
    "        elif page > 3: # just 3 pages\n",
    "            break\n",
    "\n",
    "        # Print fetched issue titles\n",
    "        print(f\"Page {page} Results:\")\n",
    "        for issue in data:\n",
    "            print(f\"- {issue['title']} (#{issue['number']})\")\n",
    "\n",
    "        # Move to the next page\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf045ae-12c6-465b-9379-bc0f5e88c6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
